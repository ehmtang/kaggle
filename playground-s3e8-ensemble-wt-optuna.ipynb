{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Playground Season 3 Episode 8\n## Problem Statement\nYou are hired by a company Gem Stones co ltd, which is a cubic zirconia manufacturer. You are provided with the dataset containing the prices and other attributes of almost 27,000 cubic zirconia (which is an inexpensive diamond alternative with many of the same qualities as a diamond). <br>\n\nThe company is earning different profits on different prize slots. You have to help the company in predicting the price for the stone on the basis of the details given in the dataset so it can distinguish between higher profitable stones and lower profitable stones so as to have a better profit share. Also, provide them with the best 5 attributes that are most important.\n\n**Description of Features**\n- Carat: Carat weight of the cubic zirconia.\n- Cut: Describe the cut quality of the cubic zirconia. Quality is increasing order Fair, Good, Very Good, Premium, Ideal.\n- Color: Colour of the cubic zirconia. With D being the best and J the worst.\n- Clarity: Cubic zirconia Clarity refers to the absence of the Inclusions and Blemishes. (In order from Best to Worst, FL = flawless, I3= level 3 inclusions) FL, IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\n- Depth: The Height of a cubic zirconia, measured from the Culet to the table, divided by its average Girdle Diameter\n- Table: The Width of the cubic zirconia's Table expressed as a Percentage of its Average Diameter.\n- Price (target): The Price of the cubic zirconia.\n- X: Length of the cubic zirconia in mm.\n- Y: Width of the cubic zirconia in mm.\n- Z: Height of the cubic zirconia in mm.","metadata":{}},{"cell_type":"markdown","source":"Reference: \nhttps://www.ganoksin.com/article/4-cs-gemstone-valuation/","metadata":{}},{"cell_type":"markdown","source":"### Overview\n\n1. Understand the shape and type of data\n2. Data Cleaning\n3. Data Exploration\n4. Feature Engineering\n5. Data Preprocessing for Model\n6. Basic Model Building\n7. Model Tuning\n8. Ensemble Model Building\n9. Results","metadata":{}},{"cell_type":"code","source":"# Libraries for EDA\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-03T20:13:15.585253Z","iopub.execute_input":"2023-03-03T20:13:15.585616Z","iopub.status.idle":"2023-03-03T20:13:15.591754Z","shell.execute_reply.started":"2023-03-03T20:13:15.585585Z","shell.execute_reply":"2023-03-03T20:13:15.590733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Data\ndf_train = pd.read_csv(\"../input/playground-series-s3e8/train.csv\")\ndf_test = pd.read_csv(\"../input/playground-series-s3e8/test.csv\")\n\n# Add the \"train\" column and merge the data\ndf_train['train'] = 1\ndf_test['train'] = 0\ndf_total = pd.concat([df_train, df_test])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:15.606448Z","iopub.execute_input":"2023-03-03T20:13:15.608536Z","iopub.status.idle":"2023-03-03T20:13:15.857533Z","shell.execute_reply.started":"2023-03-03T20:13:15.608508Z","shell.execute_reply":"2023-03-03T20:13:15.856540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Understanding the Shape and Type of Data\n<i>This involves getting a high-level understanding of the data you are working with, including its size, format, and the types of variables present.</i>\n","metadata":{}},{"cell_type":"code","source":"display(df_total.info())","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:15.859883Z","iopub.execute_input":"2023-03-03T20:13:15.860274Z","iopub.status.idle":"2023-03-03T20:13:15.917650Z","shell.execute_reply.started":"2023-03-03T20:13:15.860239Z","shell.execute_reply":"2023-03-03T20:13:15.916715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_total.nunique())","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:15.919353Z","iopub.execute_input":"2023-03-03T20:13:15.919730Z","iopub.status.idle":"2023-03-03T20:13:16.020619Z","shell.execute_reply.started":"2023-03-03T20:13:15.919692Z","shell.execute_reply":"2023-03-03T20:13:16.019631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape of training data: {df_train.shape}\")\nprint(f\"Shape of test data: {df_test.shape}\")\nprint(f\"Shape of total data: {df_total.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:16.023486Z","iopub.execute_input":"2023-03-03T20:13:16.023881Z","iopub.status.idle":"2023-03-03T20:13:16.029571Z","shell.execute_reply.started":"2023-03-03T20:13:16.023853Z","shell.execute_reply":"2023-03-03T20:13:16.028383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What does our data look like?\ndisplay(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:16.031458Z","iopub.execute_input":"2023-03-03T20:13:16.032248Z","iopub.status.idle":"2023-03-03T20:13:16.054006Z","shell.execute_reply.started":"2023-03-03T20:13:16.032211Z","shell.execute_reply":"2023-03-03T20:13:16.053015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General statistics of our dataset\ndisplay(df_total.describe())","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:16.055608Z","iopub.execute_input":"2023-03-03T20:13:16.056312Z","iopub.status.idle":"2023-03-03T20:13:16.189635Z","shell.execute_reply.started":"2023-03-03T20:13:16.056275Z","shell.execute_reply":"2023-03-03T20:13:16.188634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the target variable\ntarget = 'price'\n\n# Assign list of featured columns\nfeat_cols = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']\nnum_cols = ['carat', 'depth', 'table', 'x', 'y', 'z']\ncat_cols = ['cut', 'color', 'clarity']","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:16.191520Z","iopub.execute_input":"2023-03-03T20:13:16.192490Z","iopub.status.idle":"2023-03-03T20:13:16.198261Z","shell.execute_reply.started":"2023-03-03T20:13:16.192451Z","shell.execute_reply":"2023-03-03T20:13:16.197216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Data Cleaning\n<i>This involves identifying and handling missing or invalid data, dealing with outliers, and correcting any other errors or inconsistencies in the data.</i>\n","metadata":{}},{"cell_type":"code","source":"# Find any duplicated rows in dataset\nprint(f\"Number of duplicated rows: {df_total.duplicated().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:16.200035Z","iopub.execute_input":"2023-03-03T20:13:16.200459Z","iopub.status.idle":"2023-03-03T20:13:16.346908Z","shell.execute_reply.started":"2023-03-03T20:13:16.200417Z","shell.execute_reply":"2023-03-03T20:13:16.345718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find any missing values in dataset\nprint(\"Number of NaN values in each column:\")\nprint(df_total.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:16.348527Z","iopub.execute_input":"2023-03-03T20:13:16.348924Z","iopub.status.idle":"2023-03-03T20:13:16.396880Z","shell.execute_reply.started":"2023-03-03T20:13:16.348887Z","shell.execute_reply":"2023-03-03T20:13:16.395834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Data Exploration\n<i>This involves visualizing and summarizing the data to gain insights into its distribution, correlations, and other patterns.</i>","metadata":{}},{"cell_type":"code","source":"# Pairwise plot of numerical data, groupedby color\npercentOfSample = 0.1 # 10% of dataset\n\nsns.pairplot(data = df_train.sample(frac = percentOfSample),\n             vars = num_cols + [target],\n             hue = 'color',\n             corner = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:16.401704Z","iopub.execute_input":"2023-03-03T20:13:16.403610Z","iopub.status.idle":"2023-03-03T20:13:59.354230Z","shell.execute_reply.started":"2023-03-03T20:13:16.403581Z","shell.execute_reply":"2023-03-03T20:13:59.353130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pairwise plot of numerical data, groupedby clarity\nsns.pairplot(data = df_train.sample(frac = percentOfSample),\n             vars = num_cols + [target],\n             hue = 'clarity',\n             corner = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:13:59.355618Z","iopub.execute_input":"2023-03-03T20:13:59.356249Z","iopub.status.idle":"2023-03-03T20:14:45.793185Z","shell.execute_reply.started":"2023-03-03T20:13:59.356210Z","shell.execute_reply":"2023-03-03T20:14:45.791403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pairwise plot of numerical data, groupedby cut\nsns.pairplot(data = df_train.sample(frac = percentOfSample),     \n             vars = num_cols + [target],\n             hue = 'cut',\n             corner = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:14:45.794540Z","iopub.execute_input":"2023-03-03T20:14:45.795488Z","iopub.status.idle":"2023-03-03T20:15:27.282999Z","shell.execute_reply.started":"2023-03-03T20:14:45.795453Z","shell.execute_reply":"2023-03-03T20:15:27.281937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Depth and table do not appear to present strong correlation to price or with other features","metadata":{}},{"cell_type":"code","source":"# Correlation map of Numeric Data and Target\ndisplay(pd.DataFrame(df_train[num_cols + [target]].corr()))\n\nfig, ax = plt.subplots(figsize = (8, 6))\n    \nsns.heatmap(data = df_train[num_cols + [target]].corr(),\n            square = True,\n            vmin = -1, \n            vmax = 1, \n            annot = True,\n            yticklabels = num_cols + [target], \n            xticklabels = num_cols + [target],\n            annot_kws = {\"size\": 30 / np.sqrt(len(df_train[num_cols + [target]].corr()))},\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:27.284655Z","iopub.execute_input":"2023-03-03T20:15:27.285035Z","iopub.status.idle":"2023-03-03T20:15:27.973532Z","shell.execute_reply.started":"2023-03-03T20:15:27.284996Z","shell.execute_reply":"2023-03-03T20:15:27.972606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Depth and table offer low correlation to price and other features. \n\nHigh corrlation with carat (one of the 4C's).\n\nGeometric dimensions, 'x', 'y' and 'z' have high correlation with price. This is expected given the relationship with volume, density and weight (carat). \n\nShape (round, pear, square, etc) may also be a factor in the price of the gemstone. We can estimate it's shape given the x, y, z, carat, table and depth but may introduce additional bias into our model.","metadata":{}},{"cell_type":"code","source":"# Visualising Categorical Data\n\n# Define order of categories for each variable\ncut_order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\ncolor_order = ['D', 'E', 'F', 'G', 'H', 'I', 'J']\nclarity_order = ['I3', 'I2', 'I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF', 'FL']\n\n# Create a 1x3 grid of subplots\nfig, axs = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 5))\n\n# Bar plots\nsns.barplot(x = df_train['clarity'].value_counts().index, \n            y = df_train['clarity'].value_counts(),\n            order = clarity_order, ax = axs[0])\n\nsns.barplot(x = df_train['color'].value_counts().index, \n            y = df_train['color'].value_counts(),\n            order = color_order, ax = axs[1])\n\nsns.barplot(x = df_train['cut'].value_counts().index, \n            y = df_train['cut'].value_counts(),\n            order = cut_order, ax = axs[2])\n\n# Adjust the layout of the plot\nplt.tight_layout()\nsns.despine(trim = True)\nplt.subplots_adjust(hspace = 0.3, wspace = 0.3)\nsns.set(font_scale = 0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:27.975329Z","iopub.execute_input":"2023-03-03T20:15:27.975731Z","iopub.status.idle":"2023-03-03T20:15:28.632646Z","shell.execute_reply.started":"2023-03-03T20:15:27.975692Z","shell.execute_reply":"2023-03-03T20:15:28.631637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 3x2 grid of subplots\nfig, axs = plt.subplots(nrows = 3, ncols = 2, figsize = (15, 15))\n\n# Pointplots\nsns.pointplot(data = df_train, x = \"color\", y = \"price\", hue = \"clarity\", order = color_order, ax = axs[0,0])\nsns.pointplot(data = df_train, x = \"color\", y = \"price\", hue = \"cut\", order = color_order, ax = axs[0,1])\nsns.pointplot(data = df_train, x = \"clarity\", y = \"price\", hue = \"cut\", order = clarity_order, ax = axs[1,0])\nsns.pointplot(data = df_train, x = \"clarity\", y = \"price\", hue = \"color\", order = clarity_order, ax = axs[1,1])\nsns.pointplot(data = df_train, x = \"cut\", y = \"price\", hue = \"clarity\", order = cut_order, ax = axs[2,0])\nsns.pointplot(data = df_train, x = \"cut\", y = \"price\", hue = \"color\", order = cut_order, ax = axs[2,1])\n\n# Adjust the layout of the plot\nplt.tight_layout()\nsns.despine(trim = True)\nplt.subplots_adjust(hspace = 0.3, wspace = 0.3)\nsns.set(font_scale = 0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:28.634729Z","iopub.execute_input":"2023-03-03T20:15:28.635444Z","iopub.status.idle":"2023-03-03T20:15:49.151282Z","shell.execute_reply.started":"2023-03-03T20:15:28.635405Z","shell.execute_reply":"2023-03-03T20:15:49.150269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 1x3 grid of subplots\nfig, axs = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 5))\n\n# Boxplots\nsns.boxplot(data = df_train, x = 'clarity', y = 'price', order = clarity_order, ax = axs[0])\nsns.boxplot(data = df_train, x = 'color', y = 'price', order = color_order, ax = axs[1])\nsns.boxplot(data = df_train, x = 'cut', y = 'price', order = cut_order, ax = axs[2])\n\n# Adjust the layout of the plot\nplt.tight_layout()\nsns.despine(trim = True)\nplt.subplots_adjust(hspace = 0.3, wspace = 0.3)\nsns.set(font_scale = 0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:49.152809Z","iopub.execute_input":"2023-03-03T20:15:49.153163Z","iopub.status.idle":"2023-03-03T20:15:50.166571Z","shell.execute_reply.started":"2023-03-03T20:15:49.153129Z","shell.execute_reply":"2023-03-03T20:15:50.165604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"High numbers of outliers obseverved in box plots. This is expected given earlier analysis, as shown by the correlation of carat, x, y, and z with price.","metadata":{}},{"cell_type":"markdown","source":"### 4. Feature Engineering\n<i>This involves creating new features or transforming existing features to better represent the underlying patterns in the data.</i>","metadata":{}},{"cell_type":"markdown","source":"Geometric data shown to be highly correlated with price, \n\nSo we can estimate what the shape is given its volume, table, depth, x, y, z data. But for now let's ignore this as it may introduce new bias into our model.","metadata":{}},{"cell_type":"code","source":"# # Geometric data shown to be highly correlated with price\n# df_total['weight_g'] = df_total['carat'] / 5                    # Converting carat to grams, units: g\n# df_total['density'] = 5.65                                      # Approximate density of CZ, units: g/cm3\n# df_total['volume'] = df_total['weight_g'] / df_total['density'] # volume = weight / density\n","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:50.168225Z","iopub.execute_input":"2023-03-03T20:15:50.168899Z","iopub.status.idle":"2023-03-03T20:15:50.173303Z","shell.execute_reply.started":"2023-03-03T20:15:50.168859Z","shell.execute_reply":"2023-03-03T20:15:50.172231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Data Processing\n<i>This involves scaling, encoding, and splitting the data in preparation for model building.</i>","metadata":{}},{"cell_type":"code","source":"# Libraries required for preprocessing data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:50.174986Z","iopub.execute_input":"2023-03-03T20:15:50.175699Z","iopub.status.idle":"2023-03-03T20:15:50.186403Z","shell.execute_reply.started":"2023-03-03T20:15:50.175629Z","shell.execute_reply":"2023-03-03T20:15:50.185379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cut_order)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:50.188073Z","iopub.execute_input":"2023-03-03T20:15:50.188419Z","iopub.status.idle":"2023-03-03T20:15:50.198847Z","shell.execute_reply.started":"2023-03-03T20:15:50.188383Z","shell.execute_reply":"2023-03-03T20:15:50.197727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a LabelEncoder object\noe_cut = OrdinalEncoder(categories = [cut_order])\noe_color = OrdinalEncoder(categories = [color_order])\noe_clarity = OrdinalEncoder(categories = [clarity_order])\n\n# Encode the categorical columns using the OrdinalEncoder object\ndf_total['cut_oe'] = oe_cut.fit_transform(df_total[['cut']])\ndf_total['color_oe'] = oe_color.fit_transform(df_total[['color']])\ndf_total['clarity_oe'] = oe_clarity.fit_transform(df_total[['clarity']])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:50.200084Z","iopub.execute_input":"2023-03-03T20:15:50.200557Z","iopub.status.idle":"2023-03-03T20:15:50.477358Z","shell.execute_reply.started":"2023-03-03T20:15:50.200521Z","shell.execute_reply":"2023-03-03T20:15:50.476368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create a OneHotEncoder object and encode the categorical columns\n# oh_encode = OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n# oh_encoded_data = oh_encode.fit_transform(df_total[cat_cols])\n# oh_encoded_df = pd.DataFrame(oh_encoded_data, columns = oh_encode.get_feature_names(cat_cols))\n# oh_encoded_df.reset_index(drop = True, inplace=True)\n\n# # Merge the encoded dataframe with the original dataframe\n# df_total.reset_index(drop = True, inplace=True)\n# df_total = pd.concat([df_total, encoded_df], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:50.478606Z","iopub.execute_input":"2023-03-03T20:15:50.478989Z","iopub.status.idle":"2023-03-03T20:15:50.486325Z","shell.execute_reply.started":"2023-03-03T20:15:50.478953Z","shell.execute_reply":"2023-03-03T20:15:50.482633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are provided categorical variables that have an inherent ordering to its value. Therefore, labelEncoder is our preferred method.","metadata":{}},{"cell_type":"code","source":"# Prepare the training dataset\ndf_model = df_total.drop(['id','price', 'cut', 'color', 'clarity'], axis = 1)\ntraining_values = df_model[df_model.train == 1].drop(['train'], axis = 1)\ntest_values = df_model[df_model.train == 0].drop(['train'], axis = 1)\ntraining_target = df_total[df_total.train == 1].price","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:23:25.975845Z","iopub.execute_input":"2023-03-03T20:23:25.976231Z","iopub.status.idle":"2023-03-03T20:23:26.024690Z","shell.execute_reply.started":"2023-03-03T20:23:25.976199Z","shell.execute_reply":"2023-03-03T20:23:26.023592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into training and testing sets\ntrain_X, test_X, train_y, test_y = train_test_split(training_values, training_target, test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:50.551343Z","iopub.execute_input":"2023-03-03T20:15:50.551716Z","iopub.status.idle":"2023-03-03T20:15:50.576994Z","shell.execute_reply.started":"2023-03-03T20:15:50.551677Z","shell.execute_reply":"2023-03-03T20:15:50.576075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Basic Model Building\n<i>This involves selecting an appropriate algorithm and building a simple model to establish a baseline performance metric.</i>","metadata":{}},{"cell_type":"code","source":"# Libraries for ML models\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n# Libraries to Evaluate models\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom eli5.sklearn import PermutationImportance","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:50.578475Z","iopub.execute_input":"2023-03-03T20:15:50.578954Z","iopub.status.idle":"2023-03-03T20:15:50.584997Z","shell.execute_reply.started":"2023-03-03T20:15:50.578915Z","shell.execute_reply":"2023-03-03T20:15:50.583931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_STATE = 1\n\n# Create instances of basic models using default parameters\nxgb_model = XGBRegressor(random_state = RANDOM_STATE, objective = 'reg:squarederror')\nlgb_model = LGBMRegressor(random_state = RANDOM_STATE, objective = 'regression')\ncat_model = CatBoostRegressor(random_state = RANDOM_STATE, objective = 'RMSE', verbose = False)\n\nvoting_reg_model = VotingRegressor(estimators = [\n    ('Xtreme Gradient Boosting', xgb_model),\n    ('Light Gradient Boosting', lgb_model),\n    ('Cat Gradient Boosting', cat_model)])\n\nestimators = [\n    ('Xtreme Gradient Boosting', xgb_model),\n    ('Light Gradient Boosting', lgb_model),\n    ('Cat Gradient Boosting', cat_model),\n    ('Voting Regressor', voting_reg_model)]\n\n# Evaluate each model using cross-validation and print the results\nfor model in estimators:\n    cv_scores = cross_val_score(estimator = model[1], X = train_X, y = train_y, cv = 5, scoring = 'neg_mean_squared_error')\n    rmse_scores = np.sqrt(-cv_scores)\n    print(f\"{model[0]} Cross-Validation Mean: {rmse_scores.mean():.2f} +/- {rmse_scores.std():.2f}\\n\")   ","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:15:50.586519Z","iopub.execute_input":"2023-03-03T20:15:50.587199Z","iopub.status.idle":"2023-03-03T20:20:29.927602Z","shell.execute_reply.started":"2023-03-03T20:15:50.587164Z","shell.execute_reply":"2023-03-03T20:20:29.926394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit Model to training data\nxgb_model.fit(train_X, train_y)\nlgb_model.fit(train_X, train_y)\ncat_model.fit(train_X, train_y)\nvoting_reg_model.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:20:29.929484Z","iopub.execute_input":"2023-03-03T20:20:29.929929Z","iopub.status.idle":"2023-03-03T20:21:35.839832Z","shell.execute_reply.started":"2023-03-03T20:20:29.929887Z","shell.execute_reply":"2023-03-03T20:21:35.838854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get feature importance values and corresponding feature names\nimportance_xgb = xgb_model.feature_importances_\nimportance_lgb = lgb_model.feature_importances_\nimportance_cat = cat_model.feature_importances_\n\n# Plot feature importance for XGBoost\nsorted_idx = np.argsort(importance_xgb)\nfig, ax = plt.subplots(figsize = (12, 6))\nax.barh(range(len(sorted_idx)), importance_xgb[sorted_idx], align = 'center')\nax.set_yticks(range(len(sorted_idx)))\nax.set_yticklabels(np.array(train_X.columns)[sorted_idx])\nax.set_title('Feature Importance for XGBoost')\nplt.show()\n\n# Plot feature importance for LightGBM\nsorted_idx = np.argsort(importance_lgb)\nfig, ax = plt.subplots(figsize = (12, 6))\nax.barh(range(len(sorted_idx)), importance_lgb[sorted_idx], align = 'center')\nax.set_yticks(range(len(sorted_idx)))\nax.set_yticklabels(np.array(train_X.columns)[sorted_idx])\nax.set_title('Feature Importance for LightGBM')\nplt.show()\n\n# Plot feature importance for CatBoost\nsorted_idx = np.argsort(importance_cat)\nfig, ax = plt.subplots(figsize = (12, 6))\nax.barh(range(len(sorted_idx)), importance_cat[sorted_idx], align = 'center')\nax.set_yticks(range(len(sorted_idx)))\nax.set_yticklabels(np.array(train_X.columns)[sorted_idx])\nax.set_title('Feature Importance for CatBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:22:51.665284Z","iopub.execute_input":"2023-03-03T20:22:51.665637Z","iopub.status.idle":"2023-03-03T20:22:52.691555Z","shell.execute_reply.started":"2023-03-03T20:22:51.665607Z","shell.execute_reply":"2023-03-03T20:22:52.690583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost may be over emphasising 'y'","metadata":{}},{"cell_type":"markdown","source":"### 7. Model Tuning and Optimising Features\n<i>This involves optimising the hyperparameters of the model to improve its performance on the validation set.</i>","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:23:18.865154Z","iopub.execute_input":"2023-03-03T20:23:18.865534Z","iopub.status.idle":"2023-03-03T20:23:18.870645Z","shell.execute_reply.started":"2023-03-03T20:23:18.865502Z","shell.execute_reply":"2023-03-03T20:23:18.869540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define objective function to be minimized\ndef objective_xgb(trial, X = training_values, y = training_target):\n    \n    # Split data into training and validation sets\n    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2)\n    \n    # Define hyperparameters to optimize\n    params = {\n        'tree_method':'gpu_hist',\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0),\n        'n_estimators': 10000,\n        'max_depth': trial.suggest_int('max_depth', 2, 10),\n        'min_child_weight': trial.suggest_float('min_child_weight', 1, 300),\n        'gamma': trial.suggest_loguniform('gamma', 1e-3, 10.0),\n    }\n    \n    # Create model and fit to training data\n    model = XGBRegressor(**params)\n    model.fit(train_X, train_y, eval_set=[(test_X, test_y)], early_stopping_rounds=50, verbose=False)\n    \n    # Evaluate model on validation set\n    preds = model.predict(test_X)\n    rmse = mean_squared_error(test_y, preds, squared=False)\n    \n    return rmse\n\n# Create Optuna study and optimize hyperparameters\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective_xgb, n_trials=200)\n\n# Get best hyperparameters\nbest_trial_xgb = study.best_trial.params\n\nprint(f'Number of finished trials: {len(study.trials)}')\nprint(f'Best trial: {best_trial_xgb}')","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:23:33.089534Z","iopub.execute_input":"2023-03-03T20:23:33.090234Z","iopub.status.idle":"2023-03-03T20:37:55.755629Z","shell.execute_reply.started":"2023-03-03T20:23:33.090194Z","shell.execute_reply":"2023-03-03T20:37:55.754482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best trial: {'lambda': 0.027518478540242433, 'alpha': 0.032866241738019684, 'colsample_bytree': 0.5749802969702276, 'subsample': 0.8043266493482308, 'learning_rate': 0.050413311493018684, 'max_depth': 7, 'min_child_weight': 55.13650812717483, 'gamma': 0.056026172952691015}","metadata":{}},{"cell_type":"code","source":"# Define objective function to be minimized\ndef objective_lgbm(trial, X = training_values, y = training_target):\n    \n    # Split data into training and validation sets\n    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2)\n    \n    # Define hyperparameters to optimize\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'gbdt',\n        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n        'max_depth': trial.suggest_int('max_depth', 5, 15, step = 1),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n        'verbosity': -1,\n        'n_estimators': 10000,\n    }\n    \n    # Create model and fit to training data\n    model = LGBMRegressor(**params)\n    model.fit(train_X, train_y, eval_set=[(test_X, test_y)], early_stopping_rounds=50, verbose=False)\n \n    # Calculate RMSE on validation set\n    preds = model.predict(test_X)\n    rmse = mean_squared_error(test_y, preds, squared = False)\n    \n    # Return RMSE as the objective value to be minimized\n    return rmse\n\n# Define study and optimize hyperparameters\nstudy = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective_lgbm, n_trials = 200)\n\n# Get best hyperparameters and corresponding RMSE\nbest_trial_lgb = study.best_trial.params\n\nprint(f'Number of finished trials: {len(study.trials)}')\nprint(f'Best trial: {best_trial_lgb}')","metadata":{"execution":{"iopub.status.busy":"2023-03-03T22:17:44.073271Z","iopub.execute_input":"2023-03-03T22:17:44.073637Z","iopub.status.idle":"2023-03-03T22:50:10.670395Z","shell.execute_reply.started":"2023-03-03T22:17:44.073606Z","shell.execute_reply":"2023-03-03T22:50:10.669313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of finished trials: 200\nBest trial: {'num_leaves': 771, 'max_depth': 6, 'min_data_in_leaf': 90, 'learning_rate': 0.08125313292346216, 'feature_fraction': 0.8653153559394122, 'bagging_fraction': 0.6625570556339628, 'bagging_freq': 10, 'min_child_samples': 94, 'reg_alpha': 2.5483602114471396e-06, 'reg_lambda': 0.01891811952141336}","metadata":{}},{"cell_type":"code","source":"# Define objective function to be minimized\ndef objective_cat(trial, X = training_values, y = training_target):\n    \n    # Split data into training and validation sets\n    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n    \n    # Define hyperparameters to optimize\n    params = {\n        'n_estimators': 10000,\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 100.0),\n        'random_strength': trial.suggest_float('random_strength', 0.0, 10.0),\n        'one_hot_max_size': trial.suggest_int('one_hot_max_size', 2, 16),\n        'cat_features': [i for i in range(len(X.columns)) if X.dtypes[i] == 'category'],\n        'verbose': False,\n        'allow_writing_files': False,\n        'thread_count': -1\n    }\n    \n    # Create model and fit to training data\n    model = CatBoostRegressor(**params)\n    model.fit(train_X, train_y, eval_set = [(test_X, test_y)], early_stopping_rounds=100, verbose=False)\n    \n    # Evaluate model on validation set\n    preds = model.predict(test_X)\n    rmse = mean_squared_error(test_y, preds, squared = False)\n    \n    return rmse\n\n# Create Optuna study and optimize hyperparameters\nstudy = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective_cat, n_trials = 200)\n\n# Get best hyperparameters\nbest_trial_cat = study.best_trial.params\nprint(f'Number of finished trials: {len(study.trials)}')\nprint(f'Best trial: {best_trial_cat}')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-03T20:45:28.632356Z","iopub.execute_input":"2023-03-03T20:45:28.632734Z","iopub.status.idle":"2023-03-03T21:54:50.003810Z","shell.execute_reply.started":"2023-03-03T20:45:28.632700Z","shell.execute_reply":"2023-03-03T21:54:50.002654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best trial: {'learning_rate': 0.06885310731231294, 'depth': 6, 'l2_leaf_reg': 2.0974907315651783, 'bagging_temperature': 59.04421296914569, 'random_strength': 4.0504687744470225, 'one_hot_max_size': 11}","metadata":{}},{"cell_type":"markdown","source":"### 8. Ensemble Model Building \n<i>This involves combining multiple models to improve overall predictive performance.</i>","metadata":{}},{"cell_type":"code","source":"# Create an instance of the XGBRegressor with the best trial hyperparameters\nxgb_model = XGBRegressor(**best_trial_xgb)\nlgb_model = LGBMRegressor(**best_trial_lgb)\ncat_model = CatBoostRegressor(**best_trial_cat)\n\nvoting_reg_model = VotingRegressor(estimators = [\n    ('Xtreme Gradient Boosting', xgb_model),\n    ('Light Gradient Boosting', lgb_model),\n    ('Cat Gradient Boosting', cat_model)])\n\nestimators = [\n    ('Xtreme Gradient Boosting', xgb_model),\n    ('Light Gradient Boosting', lgb_model),\n    ('Cat Gradient Boosting', cat_model),\n    ('Voting Regressor', voting_reg_model)]\n\n# Evaluate each model using cross-validation and print the results\nfor model in estimators:\n    cv_scores = cross_val_score(estimator = model[1], X = train_X, y = train_y, cv = 5, scoring = 'neg_mean_squared_error')\n    rmse_scores = np.sqrt(-cv_scores)\n    print(f\"{model[0]} Cross-Validation Mean: {rmse_scores.mean():.2f} +/- {rmse_scores.std():.2f}\\n\") \n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-03T22:58:03.936511Z","iopub.execute_input":"2023-03-03T22:58:03.937002Z","iopub.status.idle":"2023-03-03T23:02:46.196734Z","shell.execute_reply.started":"2023-03-03T22:58:03.936962Z","shell.execute_reply":"2023-03-03T23:02:46.195505Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model on the entire training set and make predictions on the test set\nxgb_model.fit(training_values, training_target)\nlgb_model.fit(training_values, training_target)\ncat_model.fit(training_values, training_target)\nvoting_reg_model.fit(training_values, training_target)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T23:02:46.199301Z","iopub.execute_input":"2023-03-03T23:02:46.200049Z","iopub.status.idle":"2023-03-03T23:04:08.102441Z","shell.execute_reply.started":"2023-03-03T23:02:46.200004Z","shell.execute_reply":"2023-03-03T23:04:08.101036Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict y label on test data\npreds_xgb = xgb_model.predict(test_values)\npreds_lgb = lgb_model.predict(test_values)\npreds_cat = cat_model.predict(test_values)\npreds_voting_reg = voting_reg_model.predict(test_values)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T23:10:05.637011Z","iopub.execute_input":"2023-03-03T23:10:05.637399Z","iopub.status.idle":"2023-03-03T23:10:07.541016Z","shell.execute_reply.started":"2023-03-03T23:10:05.637364Z","shell.execute_reply":"2023-03-03T23:10:07.540016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9. Results\n<i>This involves evaluating the final model on a test set and presenting the results, including metrics such as accuracy, precision, recall, and F1 score.</i>\n","metadata":{}},{"cell_type":"code","source":"# Submit prediction to csv\ndf_test = pd.read_csv(\"../input/playground-series-s3e8/test.csv\")\nsubmission_xgb = pd.DataFrame({'Id': df_test.id, 'price': preds_xgb.astype(float)})\nsubmission_lgb = pd.DataFrame({'Id': df_test.id, 'price': preds_lgb.astype(float)})\nsubmission_cat = pd.DataFrame({'Id': df_test.id, 'price': preds_cat.astype(float)})\nsubmission_voting_reg = pd.DataFrame({'Id': df_test.id, 'price': preds_voting_reg.astype(float)})\n\nsubmission_xgb.to_csv('submission_xgb.csv', index = False)\nsubmission_lgb.to_csv('submission_lgb.csv', index = False)\nsubmission_cat.to_csv('submission_cat.csv', index = False)\nsubmission_voting_reg.to_csv('submission_voting_reg.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T23:10:13.682964Z","iopub.execute_input":"2023-03-03T23:10:13.683364Z","iopub.status.idle":"2023-03-03T23:10:14.776089Z","shell.execute_reply.started":"2023-03-03T23:10:13.683331Z","shell.execute_reply":"2023-03-03T23:10:14.775004Z"},"trusted":true},"execution_count":null,"outputs":[]}]}